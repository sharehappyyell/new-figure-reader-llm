from crawl4ai import AsyncWebCrawler
from typing import Optional

# è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ã‚¯ãƒ­ãƒ¼ãƒ©ãƒ¼è¨­å®šã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from config import CRAWLER_CONFIG


async def get_content_from_url(url: str) -> Optional[str]:
    """
    æŒ‡å®šã•ã‚ŒãŸURLã‹ã‚‰ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ã‚¯ãƒ­ãƒ¼ãƒ«ã—ã€Markdownå½¢å¼ã§è¿”ã™ã€‚
    """
    print(f"ğŸ”„ URLã‹ã‚‰ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’å–å¾—ã—ã¦ã„ã¾ã™: {url}")
    try:
        async with AsyncWebCrawler() as crawler:
            result = await crawler.arun(url=url, config=CRAWLER_CONFIG)
            content = result.markdown

            if not content:
                print("âŒ ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸã€‚")
                return None

            print("âœ… ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®å–å¾—å®Œäº†ã€‚")
            return content

    except Exception as e:
        print(f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        return None
